# SQL Server Connector

This custom connector extracts metadata from SQL Server databases to create a [metadata import file](https://cloud.google.com/dataplex/docs/import-metadata#components) for Google Dataplex. 

## Target objects and schemas:

The connector will extract metadata for the following database objects:
* Tables
* Views

Fully Qualified Names for entries generated by this connector confirm to the SQL Server FQN defined in the [Dataplex Documentation for FQN](https://cloud.google.com/dataplex/docs/fully-qualified-names#predefined-sqlserver)

It will not extract metadata for the following schemas:
* INFORMATION_SCHEMA
* db_accessadmin,db_backupoperator,db_datareader,db_datawriter,db_ddladmin,db_denydatareader,db_denydatawriter,db_owner,db_security,admin,guest,sys

## Prepare your SQL Server environment:

Best practise is to connect to the database using a dedicated user which has the minimum required privileges to extract metadata. 

1. Create a user in the SQL Server instance(s) and grant it the following permissions: 
    * CONNECT to the database
    * SELECT on all tables in the target database(s)
2. Add the password for the user to the Google Cloud Secret Manager in your project and note the Secret ID (format is: projects/[project-number]/secrets/[secret-name])

## Parameters
The SQL Server connector takes the following parameters:
|Parameter|Description|Required/Optional|
|---------|------------|-------------|
|target_project_id|GCP Project ID or number which will be used in the generated Dataplex Entry, Aspects and AspectTypes|REQUIRED|
|target_location_id|GCP region ID which will be used in the generated Dataplex Entry, Aspects and AspectTypes|REQUIRED|
|target_entry_group_id|The Dataplex Entry Group ID to be used in the metadata|REQUIRED|
|host|SQL Server server to connect to|REQUIRED|
|port|SQL Server host port (usually 1443)|REQUIRED|
|instancename|The SQL Server instance to connect to. If not provided the default instance will be used|OPTIONAL
|database|The SQL Server database name|REQUIRED|
|logintimeout|0-60 Allowed timeout in seconds to establish connection to SQL Server|OPTIONAL
|encrypt|True/False Encrypt connection to database|OPTIONAL
|trustservercertificate|True/False SQL Server TLS certificate or not|OPTIONAL
|hostnameincertificate|domain of host certificate|OPTIONAL
|user|Username to connect with|REQUIRED|
|password-secret|GCP Secret Manager ID holding the password for the user. Format: projects/PROJ/secrets/SECRET|REQUIRED|
|output_bucket|GCS bucket where the output file will be stored|REQUIRED|
|output_folder|Folder within the GCS bucket where the export output file will be stored|REQUIRED|

### Running the connector
There are three ways to run the connector:
1) [Run the script directly from the command line](###running-from-the-command-line) (extract metadata into GCS)
2) [Run as a container via a Dataproc Serverless job](###submitting-a-metadata-extraction-job-to-dataproc-serverless) (extract metadata into GCS)
3) [Schedule and run as a container via Workflows](###schedule-an-end-to-end-metadata-extract-and-import-with-workflows) ] (End-to-end. Extract metadata into GCS and import metadata into Dataplex)

## Running from the command line

The metadata connector can be run ad-hoc from the command line for development or testing by directly executing the main.py script.

### Prepare the environment:
1. Download the **mssql-jdbc** jar file [from Microsoft](https://docs.microsoft.com/en-us/sql/connect/jdbc/download-microsoft-jdbc-driver-for-sql-server?view=sql-server-2022)
2. Edit JAR_FILE and SPARK_JAR_PATH in [connection_jar.py](src/connection_jar.py) to match the name and location of the jar file
3. Ensure a Java Runtime Environment (JRE) is installed in your environment
4. If you don't have one set up already, create a Python virtual environment to isolate the connector.
    See [here](https://www.freecodecamp.org/news/how-to-setup-virtual-environments-in-python/) for more details but the TL;DR instructions are to run the following in your home directory:
    ```
    pip install virtualenv
    python -m venv myvenv
    source venv/bin/activate
    ```
5. Install PySpark
    ```bash
    pip3 install pyspark
    ```
6. Install all dependencies from the requirements.txt file 
    ```bash
    pip3 install -r requirements.txt
    ```
7. Ensure you have a clear network path from the machine where you will run the script to the target database server

### Required IAM Roles
- roles/secretmanager.secretAccessor
- roles/storage.objectUser

Before you run the script ensure you session is authenticated as a user which has the above roles at minimum. For example, using 
```bash
gcloud auth application-default login
```

To execute the metadata extraction run the following command (substituting appropriate values for your environment):

```shell 
python3 main.py \
--target_project_id my-gcp-project-id \
--target_location_id us-central1 \
--target_entry_group_id sqlserver \
--host the-sqlserver-server \
--port 1433 \
--database dbtoextractfrom \
--user dataplexagent \
--password-secret projects/73813454526/secrets/dataplexagent_sqlserver \
--output_bucket dataplex_connectivity_imports \
--output_folder sqlserver
```

### Output:
The connector generates a metadata extract file in JSONL format as described [in the documentation](https://cloud.google.com/dataplex/docs/import-metadata#metadata-import-file). A sample output from the SQL Server connector can be found [here](sample/sqlserver_output_sample.jsonl)

## Build a container and extract metadata using Dataproc Serverless

Building a Docker container for the connector allows it to be run from a variety of Google Cloud services including [Dataproc serverless](https://cloud.google.com/dataproc-serverless/docs) job:

### Building the container (one-time task)

Ensure you have Docker installed in your environment before you begin and that the user you run the script with has artifactregistry.repositories.uploadArtifacts privilege on the artfiact registry in your project.

1. Edit [build_and_push_docker.sh](build_and_push_docker.sh) and set the PROJECT AND REGION_ID
2. Make the script executable and run
    ```bash
    chmod a+x build_and_push_docker.sh
    ./build_and_push_docker.sh
    ``` 
    This will build a Docker container called **dataplex-sqlserver-pyspark** and store it in Artifact Registry. 
    This process can take take up to 10 minutes.

### Submitting a metadata extraction job to Dataproc serverless:
Once the container is built you can run the metadata extract with the following command (substituting appropriate values for your environment). 

Before you run please ensure:
1. You upload the **mssql-jdbc** jar file to a Google Cloud Storage location and use the path to this in the **--jars** parameter below.
2. Create a GCS bucket which will be used for Dataproc Serverless as a working directory (add to the **--deps-bucket** parameter below.
3. The service account you run the job with using **--service-account** below has the IAM roles described [here](https://cloud.google.com/dataplex/docs/import-using-workflows-custom-source#required-roles).
You can use this [script](../common_scripts/grant_SA_dataproc_roles.sh) to grant the required roles to your service account.

```shell
gcloud dataproc batches submit pyspark \
    --project=my-gcp-project-id \
    --region=us-central1 \
    --batch=0001 \
    --deps-bucket=dataplex-metadata-collection-usc1 \  
    --container-image=us-central1-docker.pkg.dev/my-gcp-project-id/docker-repo/sqlserver-pyspark@sha256:dab02ca02f60a9e12767996191b06d859b947d89490d636a34fc734d4a0b6d08 \
    --service-account=440165342669-compute@developer.gserviceaccount.com \
    --jars=gs://path/to/mssql-jdbc-9.4.1.jre8.jar  \
    --network=[Your-Network-Name] \
    main.py \
--  --target_project_id my-gcp-project-id \
      --target_location_id us-central1	\
      --target_entry_group_id XXX \
      --host the-sqlserver-server \
      --port 1433 \
      --user dataplexagent \
      --password-secret projects/73813454526/dataplexagent_sqlserver \
      --database AdventureWorksDW2019 \
      --output_bucket gs://dataplex_connectivity_imports \
      --output_folder sqlserver
```

### 3. Schedule end-to-end metadata extraction and import using Google Cloud Workflows

To run an end-to-end metadata extraction and import process, run the container via Google Cloud Workflows. 

Follow the Dataplex documentation here: [Import metadata from a custom source using Workflows](https://cloud.google.com/dataplex/docs/import-using-workflows-custom-source) and use [this yaml file](https://github.com/GoogleCloudPlatform/cloud-dataplex/blob/main/managed-connectivity/cloud-workflows/byo-connector/templates/byo-connector.yaml) as a template.


## Manually running a metadata import into Dataplex

To import a metadata import file into Dataplex, see the [Dataplex documetation](https://cloud.google.com/dataplex/docs/import-metadata#import-metadata) for full instructions about calling the API.
The [samples](/samples) directory contains an examples metadata import file and request file for callng the API
